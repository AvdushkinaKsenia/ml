Метод бустинга похож на метод бэггинга: берется множество одинаковых моделей и объединяется, чтобы получить сильного ученика. РАЗНИЦА: модели приспосабливаются к данным последовательно, т. е. каждая модель будет исправлять ошибки предыдущей.
AdaBoost представляет собой несколько последовательно конструируемых классификаторов, обрабатывающие обучающие примеры с разным w. Слабые классификаторы, каждый из которых последовательно учится на неправильно классифицированных объектах предыдущих классификаторов, последовательно группируются. Т.о. можно построить одну сильную модель.
ГРАДИЕНТНЫЙ БУСТИНГ — класс алгоритмов, представляющих бустинг как процесс градиентного спуска. В основе алгоритма лежит последовательное уточнение функции, представляющей собой линейную комбинацию базовых классификаторов, с тем чтобы минимизировать дифференцируемую функцию потерь. ГРАДИЕНТНЫЙ БУСТИНГ — метод ансамблевого обучения, который последовательно строит слабые модели (обычно DT), каждая из которых корректирует ошибки предыдущей.

XGBoost — библиотека повышения градиента с открытым исходным кодом; ориентирована на разработку эффективных и масштабируемых алгоритмов ML. совершенствованная и настраиваемая версия системы дерева решений градиентного бустинга, созданная с учетом производительности и скорости.
LightGBM — библиотека повышения градиента с открытым исходным кодом, разработанная Microsoft (2016 г.); работает быстро и эффективно, что делает ее подходящей для крупномасштабных задач. LightGBM в 6 раз быстрее, чем XGBoost.
CatBoost — библиотека повышения градиента с открытым исходным кодом, разработанная Яндексом (2017 г.), специально предназначенная для обработки категориальных данных.  обработка категориальных функций напрямую, без необходимости оперативного кодирования или другой предварительной обработки.
SketchBoost — библиотека повышения градиента с открытым исходным кодом, разработанная сбером (2023 г.). Позволяет обучать модели в десятки раз быстрее в случае размерностей порядка сотен или тысяч классов с сохранением качества. 