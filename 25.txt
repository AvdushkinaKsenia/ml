Снижение размерности служит для визуализации высокоразмерных данных, ускорения обучения моделей и подавления шума. В тех случаях, когда структура данных не укладывается в линейные подпространства, применяют нелинейные техники, сохраняющие сложные геометрические и топологические свойства исходного пространства.

1. t-SNE (t-distributed Stochastic Neighbor Embedding) основывается на вероятностном подходе: в исходном пространстве каждая пара точек связывается условной вероятностью близости, пропорциональной гауссову ядру, а в целевом низкоразмерном пространстве сходную вероятность приближают с помощью распределения Стьюдента с одной степенью свободы.
2. Isomap сочетает классический метод многомерного шкалирования (MDS) с графом ближайших соседей. Сначала строится граф, в котором соседями считаются точки в пределах заданного числа k или радиуса ε, затем между любой парой точек вычисляется кратчайший путь по этому графу, приближая геодезические расстояния на многообразии данных. На этой основе классический MDS восстанавливает координаты в низкоразмерном пространстве.
3. UMAP (Uniform Manifold Approximation and Projection) опирается на математические основания алгебраической топологии и теории многообразий. Как и t-SNE, он строит локальные графы близости, но в качестве меры близости использует взвешенные ряды вероятностей на основе экспоненциального ядра, а при проекции минимизирует кросс-энтропию между высокоразмерной и низкоразмерной структурами графов.

Существует три основных подхода к выбору признаков:
1. Фильтрационные методы оценивают значимость каждого признака независимо от модели. Они используют статистические показатели: коэффициенты корреляции, тесты на значимость, дисперсионный анализ. Предварительно удаляются признаки с близкой к нулю дисперсией, так как они не вносят информации.
2. Встроенные методы интегрируют отбор прямо в процесс обучения модели. Наиболее распространённым примером является регуляризация L1 в линейных моделях, при которой в функцию потерь добавляется сумма абсолютных значений коэффициентов.
3. Методы-оболочки (wrapper) оценивают качество подмножества признаков по результатам работы конкретного алгоритма. Они подбирают комбинацию переменных итеративно, обучая модель на разных конфигурациях: «жадный» прямой или обратный отбор, пошаговый отбор, а также более продвинутые стратегии, такие как генетические алгоритмы.
