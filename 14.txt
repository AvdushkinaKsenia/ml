Ансамблевое обучение — обучение ансамблевой модели, т.е. комбинации нескольких базовых моделей ML, каждая из которых показывает худшее качество, чем их ансамбль.
Идея: вместо попыток создать одну высокоточную модель сосредоточиться на обучении множества моделей с относительно низкой точностью. Затем прогнозы, полученные от этих "слабых" моделей, объединяются для формирования более точного и надежного результата.

Голосование — это метод, при котором ансамбль моделей принимает решение на основе голосов, выданных отдельными моделями.
Hard Voting: Это метод жёсткого голосования. В этом случае каждый классификатор в ансамбле выдает один голос за класс, и класс, который получит наибольшее количество голосов, считается предсказанием ансамбля.
Soft Voting: Это метод мягкого голосования. В отличие от жёсткого голосования, каждый классификатор в ансамбле генерирует вероятность для каждого класса. Класс с наибольшей усредненной вероятностью становится итоговым предсказанием.

Бэггинг (сокращение от bootstrap aggregation). Идея бэггинга: обучить несколько одинаковых моделей на основе различных случайных выборок из исходного набора данных. Распределение выборки неизвестно, поэтому модели получатся разными.
СЛУЧАЙНЫЕ ЛЕСА (Random Forests, RF). RF представляют собой улучшение по сравнению с алгоритмом бэггинга за счет использования небольшого трюка, который декоррелирует деревья. RF использует модифицированный алгоритм обучения дерева, который при каждом разбиении проверяет случайное подмножество объектов (вместо проверки всего пространства объектов).
Стекинг (stacking) — алгоритм ансамблирования, основные отличия которого от предыдущих состоят в следующем: может использовать алгоритмы ML разного типа, а не только из какого-то фиксированного семейства моделей ML. Результаты базовых алгоритмов объединяются в один с помощью обучаемой мета-модели, а не с помощью какого-либо обычного способа агрегации (суммирования или усреднения).
