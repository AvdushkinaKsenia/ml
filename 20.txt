Базовая архитектура многослойных НС — сети прямого распространения (Feed-Forward Networks, FNN), где информация передается в одном направлении: от входного слоя через скрытые слои к выходному слою. В таких НС нет циклов или обратных связей, что делает их простыми для понимания и реализации.
Идея, лежащая в основе НС: каждый биологический нейрон имеет несколько входов (дендритов), на основе информации с которых формируется выходной сигнал, который с помощью выхода (аксона) передается далее к органам человеческого (и не только) организма.

Многослойный перцептрон — Это классический пример полносвязной НС прямого распространения: каждый нейрон предыдущего слоя i связан с каждым нейроном следующего слоя i+1, а сигнал распространяется от входного слоя к выходному, не образуя обратных связей; процесс вычисления выходных данных НС по входным данным.
Структура:
1. Входной слой (input layer) — получает исходные данные (признаки).
2. Скрытые слои (hidden layers) — один или несколько слоёв, выполняющих нелинейные преобразования.
3. Выходной слой (output layer) — выдаёт итоговый результат (например, класс в классификации или число в регрессии).

Перцептрон Розенблатта — это одна из первых моделей искусственного нейрона, предложенная американским психологом и учёным Фрэнком Розенблаттом в 1957 году. Это простейшая форма искусственной нейронной сети, способная к обучению с учителем.
Структура: Состоит из одного слоя искусственных нейронов (простейший случай — один нейрон). Имеет входы (принимают данные), веса (настраиваются в процессе обучения) и пороговую функцию активации (обычно ступенчатая или сигмоидальная).
Принцип работы: Входные данные умножаются на веса, суммируются и подаются на функцию активации. Если сумма превышает порог — нейрон выдаёт 1, иначе 0 (или другой бинарный выход).

from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
# Загружаем данные и делим на train/test
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
# Создаём многослойный персептрон с одним скрытым слоем из 100 нейронов
model = MLPClassifier(hidden_layer_sizes=(100,),
                    activation='relu',
                    solver='adam',
                    learning_rate_init=0.001,
                    max_iter=200,
                    random_state=42)
# Обучаем и предсказываем
model.fit(X_train, y_train)
predictions = model.predict(X_test)
# Оценка точности
accuracy = model.score(X_test, y_test)


